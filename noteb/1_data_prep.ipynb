{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Datenaufbereitung fuer Surrogat & Generator\n",
    "\n",
    "Dieses Notebook fuehrt den vollstaendigen Vorverarbeitungs-Workflow fuer die\n",
    "organischen Halbleiter-Daten aus:\n",
    "\n",
    "1. Konfiguration laden und Datenquelle validieren\n",
    "2. Rohdaten pruefen, bereinigen und dokumentieren\n",
    "3. Splits sowie Normalisierung berechnen und speichern\n",
    "\n",
    "> Ergebnisdateien landen in `data/processed` und werden von Training/Generierung verwendet.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def _find_project_root(marker: str = \"src\") -> Path:\n",
    "    candidates = []\n",
    "    env_root = os.environ.get(\"PROJECT_ROOT\")\n",
    "    if env_root:\n",
    "        candidates.append(Path(env_root).expanduser())\n",
    "\n",
    "    try:\n",
    "        notebook_path = Path(__vsc_ipynb_file__).resolve()  # type: ignore[name-defined]\n",
    "        candidates.append(notebook_path.parent)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    cwd = Path().resolve()\n",
    "    candidates.append(cwd)\n",
    "    candidates.extend(cwd.parents)\n",
    "\n",
    "    for drive_letter in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\":\n",
    "        drive_root = Path(f\"{drive_letter}:/Ackern/BLLAmen\")\n",
    "        if drive_root.exists():\n",
    "            candidates.append(drive_root)\n",
    "\n",
    "\n",
    "    unique_candidates = []\n",
    "    seen = set()\n",
    "    for cand in candidates:\n",
    "        if cand is None:\n",
    "            continue\n",
    "        try:\n",
    "            resolved = cand.resolve()\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        if resolved in seen:\n",
    "            continue\n",
    "        seen.add(resolved)\n",
    "        unique_candidates.append(resolved)\n",
    "\n",
    "    for base in unique_candidates:\n",
    "        for candidate in [base, *base.parents]:\n",
    "            if (candidate / marker).exists():\n",
    "                return candidate\n",
    "\n",
    "    raise RuntimeError(f\"Could not locate project root containing {marker}/\")\n",
    "\n",
    "PROJECT_ROOT = _find_project_root()\n",
    "SRC_PATH = PROJECT_ROOT / \"src\"\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "if str(SRC_PATH) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_PATH))\n",
    "\n",
    "import json\n",
    "from typing import Sequence\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from IPython.display import display\n",
    "\n",
    "import importlib\n",
    "import src.data.dataset as dataset_module\n",
    "dataset_module = importlib.reload(dataset_module)\n",
    "from src.data.dataset import (\n",
    "    load_dataframe,\n",
    "    split_dataframe,\n",
    "    compute_normalization,\n",
    "    apply_normalization,\n",
    ")\n",
    "from src.data.featurization import mol_to_graph\n",
    "from src.utils.config import load_config\n",
    "from src.utils.log import setup_logging, get_logger\n",
    "from src.utils.plot import plot_property_histogram\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 40)\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "\n",
    "setup_logging()\n",
    "logger = get_logger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Konfiguration und Pfade\n",
    "\n",
    "Wir lesen `train_conf.yaml`, passen den Datensatzpfad bei Bedarf an und legen\n",
    "alle Ausgabeverzeichnisse an.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "CONFIG_PATH = PROJECT_ROOT / \"configs/train_conf.yaml\"\n",
    "config = load_config(CONFIG_PATH)\n",
    "logger.info(\"Konfiguration geladen: %s\", CONFIG_PATH)\n",
    "\n",
    "raw_path = (PROJECT_ROOT / config.dataset.path).resolve()\n",
    "hf_repo_id = \"n0w0f/qm9-csv\"\n",
    "hf_filename = \"qm9_dataset.csv\"\n",
    "hf_local_default = PROJECT_ROOT / \"data/raw\" / hf_filename\n",
    "\n",
    "if not raw_path.exists():\n",
    "    logger.warning(\"Konfigurierter Datensatz fehlt: %s\", raw_path)\n",
    "    download_success = False\n",
    "    try:\n",
    "        from huggingface_hub import hf_hub_download  # type: ignore[import]\n",
    "\n",
    "        hf_downloaded = hf_hub_download(repo_id=hf_repo_id, filename=hf_filename, repo_type=\"dataset\")\n",
    "        logger.info(\"QM9 Dataset von Hugging Face geladen: %s\", hf_downloaded)\n",
    "        hf_df = pd.read_csv(hf_downloaded)\n",
    "        if \"smiles\" not in hf_df.columns:\n",
    "            raise KeyError(\"QM9 Dataset enthaelt keine 'smiles'-Spalte\")\n",
    "        hf_local_default.parent.mkdir(parents=True, exist_ok=True)\n",
    "        hf_df.to_csv(hf_local_default, index=False)\n",
    "        raw_path = hf_local_default.resolve()\n",
    "        download_success = True\n",
    "        logger.info(\"QM9 Dataset lokal gespeichert: %s\", raw_path)\n",
    "    except ImportError:\n",
    "        logger.error(\"Bitte `pip install huggingface_hub[fsspec]` ausfuehren, um das QM9 Dataset zu laden.\")\n",
    "    except Exception as exc:\n",
    "        logger.exception(\"Fehler beim Laden des QM9 Datasets: %s\", exc)\n",
    "\n",
    "    if not download_success:\n",
    "        fallback_candidates = [\n",
    "            hf_local_default,\n",
    "            PROJECT_ROOT / \"data/raw/osc_data.csv\",\n",
    "            PROJECT_ROOT / \"data/osc_train.csv\",\n",
    "        ]\n",
    "        for candidate in fallback_candidates:\n",
    "            if candidate.exists():\n",
    "                logger.warning(\"Verwende Fallback-Dataset: %s\", candidate)\n",
    "                raw_path = candidate.resolve()\n",
    "                download_success = True\n",
    "                break\n",
    "        if not download_success:\n",
    "            raise FileNotFoundError(f\"Kein Datensatz gefunden fuer {raw_path}\")\n",
    "\n",
    "try:\n",
    "    config.dataset.path = str(raw_path.relative_to(PROJECT_ROOT))\n",
    "except ValueError:\n",
    "    config.dataset.path = str(raw_path)\n",
    "\n",
    "PROCESSED_DIR = PROJECT_ROOT / \"data/processed\"\n",
    "PLOTS_DIR = PROCESSED_DIR / \"figures\"\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "display({\n",
    "    \"dataset_path\": config.dataset.path,\n",
    "    \"target_columns\": list(config.dataset.target_columns),\n",
    "    \"val_fraction\": float(config.dataset.val_fraction),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Rohdaten laden\n",
    "\n",
    "Wir lesen die Datei ein, validieren Pflichtspalten und schauen auf die ersten Zeilen.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_df = load_dataframe(raw_path)\n",
    "logger.info(\"Rohdaten gelesen: %d Zeilen, %d Spalten\", len(raw_df), raw_df.shape[1])\n",
    "\n",
    "expected_cols = {\"smiles\"} | set(config.dataset.target_columns)\n",
    "missing_cols = expected_cols.difference(raw_df.columns)\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Rohdaten fehlen Spalten: {sorted(missing_cols)}\")\n",
    "\n",
    "raw_df = raw_df.loc[:, [\"smiles\"] + list(config.dataset.target_columns)].copy()\n",
    "raw_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SMILES und Zielgroessen bereinigen\n",
    "\n",
    "Wir entfernen invalide SMILES, Zeilen mit fehlenden Targets und Duplikate.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sanitize_dataframe(df: pd.DataFrame, target_cols: Sequence[str]) -> pd.DataFrame:\n",
    "    working = df.copy()\n",
    "    working[\"smiles\"] = working[\"smiles\"].astype(str).str.strip()\n",
    "\n",
    "    mols = working[\"smiles\"].apply(Chem.MolFromSmiles)\n",
    "    invalid_mask = mols.isna()\n",
    "    if invalid_mask.any():\n",
    "        logger.warning(\"Ignoriere %d invalide SMILES\", int(invalid_mask.sum()))\n",
    "    working = working[~invalid_mask].copy()\n",
    "\n",
    "    missing_mask = working[target_cols].isna().any(axis=1)\n",
    "    if missing_mask.any():\n",
    "        logger.warning(\"Ignoriere %d Zeilen mit fehlenden Zielwerten\", int(missing_mask.sum()))\n",
    "    working = working[~missing_mask].copy()\n",
    "\n",
    "    before = len(working)\n",
    "    working = working.drop_duplicates(subset=\"smiles\").reset_index(drop=True)\n",
    "    removed = before - len(working)\n",
    "    if removed:\n",
    "        logger.info(\"Entfernte %d Duplikate anhand SMILES\", removed)\n",
    "\n",
    "    return working\n",
    "\n",
    "clean_df = sanitize_dataframe(raw_df, config.dataset.target_columns)\n",
    "logger.info(\"Bereinigte Tabelle: %d Zeilen\", len(clean_df))\n",
    "clean_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Zielgroessen analysieren\n",
    "\n",
    "Deskriptive Statistik und Histogramme fuer die Targets.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "summary = clean_df[config.dataset.target_columns].describe().T\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "created_figures = {}\n",
    "for target in config.dataset.target_columns:\n",
    "    fig = plot_property_histogram(\n",
    "        clean_df[target].to_numpy(dtype=float),\n",
    "        title=f\"Verteilung {target}\",\n",
    "        xlabel=f\"{target} [eV]\",\n",
    "        save_path=str(PLOTS_DIR / f\"{target.lower()}_hist.png\"),\n",
    "    )\n",
    "    if fig is not None:\n",
    "        created_figures[target] = fig\n",
    "created_figures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Graph-Featurisierung testen\n",
    "\n",
    "Mit Stichproben pruefen wir die Ausgabe von `mol_to_graph`.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample = clean_df.sample(n=min(len(clean_df), 3), random_state=42)\n",
    "graph_checks = []\n",
    "for _, row in sample.iterrows():\n",
    "    data = mol_to_graph(row[\"smiles\"], y=row[config.dataset.target_columns].to_numpy(dtype=float))\n",
    "    graph_checks.append(\n",
    "        {\n",
    "            \"smiles\": row[\"smiles\"],\n",
    "            \"num_nodes\": int(data.num_nodes),\n",
    "            \"num_edges\": int(data.num_edges),\n",
    "            \"target_shape\": tuple(data.y.shape) if data.y is not None else None,\n",
    "        }\n",
    "    )\n",
    "graph_checks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train/Val/Test Split speichern\n",
    "\n",
    "Deterministischer Split mit den Helfern aus `src.data.dataset`.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "split = split_dataframe(\n",
    "    clean_df,\n",
    "    val_fraction=float(config.dataset.val_fraction),\n",
    "    test_fraction=0.1,\n",
    "    seed=42,\n",
    ")\n",
    "split_sizes = {name: len(getattr(split, name)) for name in (\"train\", \"val\", \"test\")}\n",
    "split_sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "base_name = Path(config.dataset.path).stem\n",
    "split_paths = {}\n",
    "for key in (\"train\", \"val\", \"test\"):\n",
    "    frame = getattr(split, key)\n",
    "    if frame.empty:\n",
    "        logger.warning(\"Split '%s' ist leer und wird nicht gespeichert.\", key)\n",
    "        continue\n",
    "    out_path = PROCESSED_DIR / f\"{base_name}_{key}.csv\"\n",
    "    frame.to_csv(out_path, index=False)\n",
    "    split_paths[key] = str(out_path.relative_to(PROJECT_ROOT))\n",
    "logger.info(\"Persistierte Splits: %s\", split_paths)\n",
    "split_paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Normalisierung berechnen\n",
    "\n",
    "Mean/Std werden fuer die spaetere Inferenz abgelegt.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "if split.train.empty:\n",
    "    raise RuntimeError(\"Train-Split ist leer; pruefe Datensatz oder Split-Parameter.\")\n",
    "\n",
    "norm_stats = compute_normalization(split.train, config.dataset.target_columns)\n",
    "norm_payload = {\n",
    "    \"mean\": norm_stats.mean.to_dict(),\n",
    "    \"std\": norm_stats.std.to_dict(),\n",
    "}\n",
    "norm_path = PROCESSED_DIR / f\"{base_name}_normalization.json\"\n",
    "with norm_path.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "    json.dump(norm_payload, fh, indent=2)\n",
    "\n",
    "normalized_preview = apply_normalization(\n",
    "    split.train.head(3),\n",
    "    norm_stats,\n",
    "    config.dataset.target_columns,\n",
    ")\n",
    "display(norm_payload)\n",
    "normalized_preview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Zusammenfassung\n",
    "\n",
    "- Rohdaten validiert und bereinigt\n",
    "- Splits + Normalisierung in `data/processed` abgelegt\n",
    "- Histogramme in `data/processed/figures` gespeichert\n",
    "\n",
    "Weiter geht es mit `02_surrogate_training.ipynb` oder den Skripten in `src/`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}